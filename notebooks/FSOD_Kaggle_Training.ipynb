{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419599cc",
   "metadata": {},
   "source": [
    "## Step 1: Configuration\n",
    "\n",
    "Update `DATASET_NAME` to match your Kaggle dataset name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5233fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ============ CONFIGURATION ============\n",
    "DATASET_NAME = \"coco-style-shortlisting-set\"  # ðŸ‘ˆ ONLY the dataset name, NOT the full path\n",
    "NUM_EPISODES = 100                             # ðŸ‘ˆ Change to 1000+ for real training\n",
    "DEVICE = \"cuda\"                                # GPU is always available on Kaggle\n",
    "# =======================================\n",
    "\n",
    "print(\"âœ… Configuration loaded\")\n",
    "print(f\"   Dataset: {DATASET_NAME}\")\n",
    "print(f\"   Episodes: {NUM_EPISODES}\")\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "print(f\"   Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7664946",
   "metadata": {},
   "source": [
    "## Step 2: Clone FSOD Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cf3a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“¥ Setting up FSOD repository...\")\n",
    "os.chdir(\"/kaggle/working\")\n",
    "\n",
    "# Clone repository\n",
    "if not os.path.exists(\"fsod\"):\n",
    "    print(\"   Cloning repository...\")\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"clone\", \"--depth\", \"1\", \n",
    "         \"https://github.com/yourusername/fsod.git\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    if result.returncode != 0:\n",
    "        print(f\"âŒ Clone failed: {result.stderr}\")\n",
    "        print(\"   Make sure to replace 'yourusername' with your GitHub username\")\n",
    "    else:\n",
    "        print(\"   âœ“ Repository cloned\")\n",
    "else:\n",
    "    print(\"   âœ“ Repository already exists\")\n",
    "\n",
    "os.chdir(\"fsod\")\n",
    "print(f\"\\nâœ… Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39717424",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ae721",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“¦ Installing dependencies...\")\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", \"requirements.txt\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"âœ… All dependencies installed successfully\")\n",
    "else:\n",
    "    print(f\"âŒ Installation failed: {result.stderr}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c86215",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f227601",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Š Preparing dataset...\\n\")\n",
    "\n",
    "dataset_path = Path(f\"/kaggle/input/{DATASET_NAME}\")\n",
    "print(f\"Looking for dataset at: {dataset_path}\")\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    print(f\"\\nâŒ ERROR: Dataset not found!\")\n",
    "    print(f\"\\nPossible causes:\")\n",
    "    print(f\"  1. Wrong dataset name: '{DATASET_NAME}'\")\n",
    "    print(f\"  2. Dataset not added to notebook\")\n",
    "    print(f\"\\nFix: Click 'Add Data' â†’ Search your dataset â†’ Click 'Add'\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Check if data folder exists in dataset\n",
    "data_subfolder = dataset_path / \"data\"\n",
    "if data_subfolder.exists():\n",
    "    source_path = data_subfolder\n",
    "    print(f\"Found 'data' subfolder in dataset\")\n",
    "else:\n",
    "    source_path = dataset_path\n",
    "    print(f\"Using dataset root as source\")\n",
    "\n",
    "# Copy JSON files\n",
    "print(\"\\nCopying COCO annotations...\")\n",
    "for file in [\"train_coco.json\", \"val_coco.json\"]:\n",
    "    src = source_path / file\n",
    "    dst = Path(\"data\") / file\n",
    "    if src.exists():\n",
    "        shutil.copy(src, dst)\n",
    "        print(f\"   âœ“ {file}\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Missing: {file} (will continue)\")\n",
    "\n",
    "# Copy image directories\n",
    "print(\"\\nCopying images...\")\n",
    "for dir_name in [\"train_images\", \"val_images\"]:\n",
    "    src = source_path / dir_name\n",
    "    if src.exists():\n",
    "        shutil.copytree(src, f\"data/{dir_name}\", dirs_exist_ok=True)\n",
    "        count = len(list(Path(f\"data/{dir_name}\").glob(\"*\")))\n",
    "        print(f\"   âœ“ {dir_name}: {count} images\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Missing: {dir_name}\")\n",
    "\n",
    "# List what was copied\n",
    "print(\"\\nðŸ“‚ Data directory contents:\")\n",
    "data_dir = Path(\"data\")\n",
    "for item in sorted(data_dir.iterdir()):\n",
    "    if item.is_dir():\n",
    "        count = len(list(item.glob(\"*\")))\n",
    "        print(f\"   ðŸ“ {item.name}/ ({count} items)\")\n",
    "    else:\n",
    "        size_mb = item.stat().st_size / 1e6\n",
    "        print(f\"   ðŸ“„ {item.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(\"\\nâœ… Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb76e41",
   "metadata": {},
   "source": [
    "## Step 5: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ea6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"ðŸŽ® GPU Information:\")\n",
    "print(f\"   CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"   GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    memory_gb = props.total_memory / 1e9\n",
    "    print(f\"   Memory: {memory_gb:.1f} GB\")\n",
    "    print(f\"   CUDA Capability: {props.major}.{props.minor}\")\n",
    "    print(f\"\\nâœ… GPU ready for training!\")\n",
    "else:\n",
    "    print(f\"\\nâŒ No GPU detected!\")\n",
    "    print(f\"   Go to top right â†’ Accelerator â†’ Select 'GPU'\")\n",
    "    print(f\"   Then restart this notebook\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cbe362",
   "metadata": {},
   "source": [
    "## Step 6: Train Model\n",
    "\n",
    "This step trains the FSOD model on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362bcc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ‹ï¸  STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Episodes: {NUM_EPISODES}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train\n",
    "train_cmd = f\"python train.py --device {DEVICE} --num_episodes {NUM_EPISODES} --pretrained\"\n",
    "exit_code = os.system(train_cmd)\n",
    "\n",
    "if exit_code == 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âœ… TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Model saved to: checkpoints/best_model.pth\")\n",
    "    print(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âŒ TRAINING FAILED!\")\n",
    "    print(\"=\"*70)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74129ed2",
   "metadata": {},
   "source": [
    "## Step 7: Test Inference (Single Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c34c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸŽ¯ Testing inference on single image...\\n\")\n",
    "\n",
    "# Get support images from Kaggle dataset (only JPG files)\n",
    "support_dir = Path(\"/kaggle/input/sample-set-02-crops/output/Pond-2\")\n",
    "support_images = sorted([img for img in support_dir.glob(\"*.jpg\") if img.is_file()])\n",
    "\n",
    "# Get query image\n",
    "query_img = Path(\"/kaggle/input/shortlistingdataset/shortlisting-dataset-tiff/shortlisting-dataset-tiff/GC01PS03S0016.jpg\")\n",
    "\n",
    "print(f\"Found {len(support_images)} support images (JPG only)\")\n",
    "print(f\"Query image: {query_img.name}\")\n",
    "\n",
    "if len(support_images) >= 2 and query_img.exists():\n",
    "    # Prepare support images as space-separated paths\n",
    "    support_imgs = \" \".join([str(img) for img in support_images[:5]])  # Use up to 5 support images\n",
    "    \n",
    "    print(f\"\\nSupport images ({min(5, len(support_images))} selected):\")\n",
    "    for img in support_images[:5]:\n",
    "        print(f\"   â€¢ {img.name}\")\n",
    "    print(f\"\\nQuery image: {query_img.name}\")\n",
    "    \n",
    "    # Run inference\n",
    "    cmd = f\"\"\"python inference.py --mode single \\\n",
    "        --model_path checkpoints/best_model.pth \\\n",
    "        --support_img {support_imgs} \\\n",
    "        --query_image {str(query_img)} \\\n",
    "        --output_dir output \\\n",
    "        --device {DEVICE} \\\n",
    "        --score_threshold 0.3\"\"\"\n",
    "    \n",
    "    os.system(cmd)\n",
    "    print(\"\\nâœ… Inference test complete!\")\n",
    "else:\n",
    "    if len(support_images) < 2:\n",
    "        print(f\"\\nâš ï¸  Not enough support images\")\n",
    "        print(f\"   Found: {len(support_images)} JPG files in {support_dir}\")\n",
    "    if not query_img.exists():\n",
    "        print(f\"\\nâš ï¸  Query image not found\")\n",
    "        print(f\"   Expected: {query_img}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7ceb7",
   "metadata": {},
   "source": [
    "## Step 8: Batch Inference (All Validation Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“Š Running batch inference...\\n\")\n",
    "\n",
    "# Use first 3 training images as support set\n",
    "train_images = sorted(Path(\"data/train_images\").glob(\"*.jpg\"))[:3]\n",
    "support_imgs = \" \".join([str(img) for img in train_images])\n",
    "\n",
    "print(f\"Support set: {len(train_images)} images\")\n",
    "for img in train_images:\n",
    "    print(f\"   â€¢ {img.name}\")\n",
    "\n",
    "print(f\"\\nProcessing: data/val_images/\")\n",
    "\n",
    "# Run batch inference\n",
    "cmd = f\"\"\"python inference.py --mode batch \\\n",
    "    --model_path checkpoints/best_model.pth \\\n",
    "    --support_img {support_imgs} \\\n",
    "    --query_dir data/val_images/ \\\n",
    "    --output_csv results.csv \\\n",
    "    --device {DEVICE}\"\"\"\n",
    "\n",
    "os.system(cmd)\n",
    "\n",
    "# Show results\n",
    "results_csv = Path(\"results.csv\")\n",
    "if results_csv.exists():\n",
    "    df = pd.read_csv(\"results.csv\")\n",
    "    print(f\"\\nâœ… Batch inference complete!\")\n",
    "    print(f\"\\nðŸ“ˆ Results:\")\n",
    "    print(f\"   Total detections: {len(df)}\")\n",
    "    print(f\"   Unique images: {df['filename'].nunique()}\")\n",
    "    print(f\"   Average score: {df['similarity_score'].mean():.4f}\")\n",
    "    print(f\"\\n   Top 10 detections:\")\n",
    "    print(df.nlargest(10, 'similarity_score')[['filename', 'class_name', 'similarity_score']].to_string(index=False))\n",
    "else:\n",
    "    print(\"âš ï¸  Results file not created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd201d",
   "metadata": {},
   "source": [
    "## Step 9: Download Results\n",
    "\n",
    "Your trained model and results are ready to download!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e803d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“¥ FILES READY FOR DOWNLOAD\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check what files exist\n",
    "files_info = []\n",
    "\n",
    "# Model checkpoint\n",
    "model_file = Path(\"checkpoints/best_model.pth\")\n",
    "if model_file.exists():\n",
    "    size_mb = model_file.stat().st_size / 1e6\n",
    "    files_info.append((\"âœ“ Model Checkpoint\", \"checkpoints/best_model.pth\", f\"{size_mb:.1f} MB\"))\n",
    "\n",
    "# Inference outputs\n",
    "output_dir = Path(\"output\")\n",
    "if output_dir.exists():\n",
    "    count = len(list(output_dir.glob(\"*\")))\n",
    "    size_mb = sum(f.stat().st_size for f in output_dir.rglob(\"*\")) / 1e6\n",
    "    files_info.append((f\"âœ“ Inference Output\", \"output/\", f\"{count} files, {size_mb:.1f} MB\"))\n",
    "\n",
    "# CSV results\n",
    "csv_file = Path(\"results.csv\")\n",
    "if csv_file.exists():\n",
    "    size_mb = csv_file.stat().st_size / 1e6\n",
    "    files_info.append((\"âœ“ CSV Results\", \"results.csv\", f\"{size_mb:.1f} MB\"))\n",
    "\n",
    "# Print table\n",
    "print(\"\\nAvailable files:\")\n",
    "for name, path, size in files_info:\n",
    "    print(f\"   {name:<25} {path:<30} {size}\")\n",
    "\n",
    "print(\"\\nðŸ“ How to download:\")\n",
    "print(\"   1. Click 'Output' tab on the right side\")\n",
    "print(\"   2. Download the 'fsod' folder\")\n",
    "print(\"   3. Extract and use locally\")\n",
    "\n",
    "print(\"\\nâœ… Training & inference complete!\")\n",
    "print(f\"   Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
