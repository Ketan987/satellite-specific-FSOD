{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419599cc",
   "metadata": {},
   "source": [
    "## Step 1: Configuration\n",
    "\n",
    "Update `DATASET_NAME` to match your Kaggle dataset name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5233fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ============ CONFIGURATION ============\n",
    "DATASET_NAME = \"fsod-coco-data\"  # üëà CHANGE TO YOUR KAGGLE DATASET NAME\n",
    "NUM_EPISODES = 100               # üëà Change to 1000+ for real training\n",
    "DEVICE = \"cuda\"                  # GPU is always available on Kaggle\n",
    "# =======================================\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Dataset: {DATASET_NAME}\")\n",
    "print(f\"   Episodes: {NUM_EPISODES}\")\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "print(f\"   Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7664946",
   "metadata": {},
   "source": [
    "## Step 2: Clone FSOD Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cf3a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì• Setting up FSOD repository...\")\n",
    "os.chdir(\"/kaggle/working\")\n",
    "\n",
    "# Clone repository\n",
    "if not os.path.exists(\"fsod\"):\n",
    "    print(\"   Cloning repository...\")\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"clone\", \"--depth\", \"1\", \n",
    "         \"https://github.com/yourusername/fsod.git\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    if result.returncode != 0:\n",
    "        print(f\"‚ùå Clone failed: {result.stderr}\")\n",
    "        print(\"   Make sure to replace 'yourusername' with your GitHub username\")\n",
    "    else:\n",
    "        print(\"   ‚úì Repository cloned\")\n",
    "else:\n",
    "    print(\"   ‚úì Repository already exists\")\n",
    "\n",
    "os.chdir(\"fsod\")\n",
    "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39717424",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ae721",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ Installing dependencies...\")\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", \"requirements.txt\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úÖ All dependencies installed successfully\")\n",
    "else:\n",
    "    print(f\"‚ùå Installation failed: {result.stderr}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c86215",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f227601",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Preparing dataset...\\n\")\n",
    "\n",
    "dataset_path = Path(f\"/kaggle/input/{DATASET_NAME}\")\n",
    "print(f\"Looking for dataset at: {dataset_path}\")\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: Dataset not found!\")\n",
    "    print(f\"\\nPossible causes:\")\n",
    "    print(f\"  1. Wrong dataset name: '{DATASET_NAME}'\")\n",
    "    print(f\"  2. Dataset not added to notebook\")\n",
    "    print(f\"\\nFix: Click 'Add Data' ‚Üí Search your dataset ‚Üí Click 'Add'\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Copy JSON files\n",
    "print(\"\\nCopying COCO annotations...\")\n",
    "for file in [\"train_coco.json\", \"val_coco.json\"]:\n",
    "    src = dataset_path / file\n",
    "    dst = Path(\"data\") / file\n",
    "    if src.exists():\n",
    "        shutil.copy(src, dst)\n",
    "        print(f\"   ‚úì {file}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Missing: {file}\")\n",
    "\n",
    "# Copy image directories\n",
    "print(\"\\nCopying images...\")\n",
    "for dir_name in [\"train_images\", \"val_images\"]:\n",
    "    src = dataset_path / dir_name\n",
    "    if src.exists():\n",
    "        shutil.copytree(src, f\"data/{dir_name}\", dirs_exist_ok=True)\n",
    "        count = len(list(Path(f\"data/{dir_name}\").glob(\"*\")))\n",
    "        print(f\"   ‚úì {dir_name}: {count} images\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Missing: {dir_name}\")\n",
    "\n",
    "print(\"\\n‚úÖ Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb76e41",
   "metadata": {},
   "source": [
    "## Step 5: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ea6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"üéÆ GPU Information:\")\n",
    "print(f\"   CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"   GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    memory_gb = props.total_memory / 1e9\n",
    "    print(f\"   Memory: {memory_gb:.1f} GB\")\n",
    "    print(f\"   CUDA Capability: {props.major}.{props.minor}\")\n",
    "    print(f\"\\n‚úÖ GPU ready for training!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå No GPU detected!\")\n",
    "    print(f\"   Go to top right ‚Üí Accelerator ‚Üí Select 'GPU'\")\n",
    "    print(f\"   Then restart this notebook\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cbe362",
   "metadata": {},
   "source": [
    "## Step 6: Train Model\n",
    "\n",
    "This step trains the FSOD model on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362bcc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèãÔ∏è  STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Episodes: {NUM_EPISODES}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train\n",
    "train_cmd = f\"python train.py --device {DEVICE} --num_episodes {NUM_EPISODES} --pretrained\"\n",
    "exit_code = os.system(train_cmd)\n",
    "\n",
    "if exit_code == 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Model saved to: checkpoints/best_model.pth\")\n",
    "    print(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ùå TRAINING FAILED!\")\n",
    "    print(\"=\"*70)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74129ed2",
   "metadata": {},
   "source": [
    "## Step 7: Test Inference (Single Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c34c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ Testing inference on single image...\\n\")\n",
    "\n",
    "# Get sample images\n",
    "train_images = sorted(Path(\"data/train_images\").glob(\"*.jpg\"))[:5]\n",
    "val_images = sorted(Path(\"data/val_images\").glob(\"*.jpg\"))[:3]\n",
    "\n",
    "print(f\"Found {len(train_images)} training images\")\n",
    "print(f\"Found {len(val_images)} validation images\")\n",
    "\n",
    "if len(train_images) >= 2 and len(val_images) >= 1:\n",
    "    # Prepare support and query images\n",
    "    support_imgs = \" \".join([str(img) for img in train_images[:2]])\n",
    "    query_img = str(val_images[0])\n",
    "    \n",
    "    print(f\"\\nSupport images: {train_images[0].name}, {train_images[1].name}\")\n",
    "    print(f\"Query image: {val_images[0].name}\")\n",
    "    \n",
    "    # Run inference\n",
    "    cmd = f\"\"\"python inference.py --mode single \\\n",
    "        --model_path checkpoints/best_model.pth \\\n",
    "        --support_img {support_imgs} \\\n",
    "        --query_image {query_img} \\\n",
    "        --output_dir output \\\n",
    "        --device {DEVICE} \\\n",
    "        --score_threshold 0.3\"\"\"\n",
    "    \n",
    "    os.system(cmd)\n",
    "    print(\"\\n‚úÖ Inference test complete!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Not enough images for inference test\")\n",
    "    print(f\"   Need: 2 support + 1 query\")\n",
    "    print(f\"   Have: {len(train_images)} support + {len(val_images)} query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7ceb7",
   "metadata": {},
   "source": [
    "## Step 8: Batch Inference (All Validation Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Running batch inference...\\n\")\n",
    "\n",
    "# Use first 3 training images as support set\n",
    "train_images = sorted(Path(\"data/train_images\").glob(\"*.jpg\"))[:3]\n",
    "support_imgs = \" \".join([str(img) for img in train_images])\n",
    "\n",
    "print(f\"Support set: {len(train_images)} images\")\n",
    "for img in train_images:\n",
    "    print(f\"   ‚Ä¢ {img.name}\")\n",
    "\n",
    "print(f\"\\nProcessing: data/val_images/\")\n",
    "\n",
    "# Run batch inference\n",
    "cmd = f\"\"\"python inference.py --mode batch \\\n",
    "    --model_path checkpoints/best_model.pth \\\n",
    "    --support_img {support_imgs} \\\n",
    "    --query_dir data/val_images/ \\\n",
    "    --output_csv results.csv \\\n",
    "    --device {DEVICE}\"\"\"\n",
    "\n",
    "os.system(cmd)\n",
    "\n",
    "# Show results\n",
    "results_csv = Path(\"results.csv\")\n",
    "if results_csv.exists():\n",
    "    df = pd.read_csv(\"results.csv\")\n",
    "    print(f\"\\n‚úÖ Batch inference complete!\")\n",
    "    print(f\"\\nüìà Results:\")\n",
    "    print(f\"   Total detections: {len(df)}\")\n",
    "    print(f\"   Unique images: {df['filename'].nunique()}\")\n",
    "    print(f\"   Average score: {df['similarity_score'].mean():.4f}\")\n",
    "    print(f\"\\n   Top 10 detections:\")\n",
    "    print(df.nlargest(10, 'similarity_score')[['filename', 'class_name', 'similarity_score']].to_string(index=False))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Results file not created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd201d",
   "metadata": {},
   "source": [
    "## Step 9: Download Results\n",
    "\n",
    "Your trained model and results are ready to download!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e803d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üì• FILES READY FOR DOWNLOAD\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check what files exist\n",
    "files_info = []\n",
    "\n",
    "# Model checkpoint\n",
    "model_file = Path(\"checkpoints/best_model.pth\")\n",
    "if model_file.exists():\n",
    "    size_mb = model_file.stat().st_size / 1e6\n",
    "    files_info.append((\"‚úì Model Checkpoint\", \"checkpoints/best_model.pth\", f\"{size_mb:.1f} MB\"))\n",
    "\n",
    "# Inference outputs\n",
    "output_dir = Path(\"output\")\n",
    "if output_dir.exists():\n",
    "    count = len(list(output_dir.glob(\"*\")))\n",
    "    size_mb = sum(f.stat().st_size for f in output_dir.rglob(\"*\")) / 1e6\n",
    "    files_info.append((f\"‚úì Inference Output\", \"output/\", f\"{count} files, {size_mb:.1f} MB\"))\n",
    "\n",
    "# CSV results\n",
    "csv_file = Path(\"results.csv\")\n",
    "if csv_file.exists():\n",
    "    size_mb = csv_file.stat().st_size / 1e6\n",
    "    files_info.append((\"‚úì CSV Results\", \"results.csv\", f\"{size_mb:.1f} MB\"))\n",
    "\n",
    "# Print table\n",
    "print(\"\\nAvailable files:\")\n",
    "for name, path, size in files_info:\n",
    "    print(f\"   {name:<25} {path:<30} {size}\")\n",
    "\n",
    "print(\"\\nüìù How to download:\")\n",
    "print(\"   1. Click 'Output' tab on the right side\")\n",
    "print(\"   2. Download the 'fsod' folder\")\n",
    "print(\"   3. Extract and use locally\")\n",
    "\n",
    "print(\"\\n‚úÖ Training & inference complete!\")\n",
    "print(f\"   Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
